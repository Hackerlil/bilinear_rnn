\documentclass[a4paper,11pt]{article}
 
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[ansinew]{inputenc}
\usepackage{bbold}
\usepackage{bold-extra}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{gensymb}
\usepackage{mathtools}
\usepackage{subfigure}
\usepackage{theorem}
\usepackage{bm}
\usepackage{xcolor}
\usepackage[unicode]{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{lipsum}
\usetikzlibrary{positioning}
\usepackage{tikz}
\usepackage{empheq}
\usepackage{booktabs}
\usepackage{authblk}
\usepackage{cals}
%\usepackage{3dplot} %requires 3dplot.sty to be in same directory, or in your LaTeX installation
%\usepackage{amsmath,amsfonts,amssymb,amsthm,epsfig,epstopdf,titling,url,array}
\usepackage{xstring}

%\theoremstyle{definition}
%\newtheorem{defn}{Definition}[section]
%\newtheorem{conj}{Conjecture}[section]
%\newtheorem{exmp}{Example}[section]
\makeatletter

\renewcommand\cals@issue@row{%
\nointerlineskip
\setbox0=\vtop{\hbox to \textwidth{\hskip\leftskip \box\cals@current@cs \hskip\rightskip}}%
  \ht0=0pt \box0
\nointerlineskip
\hbox to \textwidth{\hskip\leftskip\hbox{\cals@issue@rowsep}\hskip\rightskip}%
\nointerlineskip
\hbox to \textwidth{\hskip\leftskip \box\cals@current@row \hskip\rightskip}%
\let\cals@last@rs@below=\cals@current@rs@below
\let\cals@last@context=\cals@current@context}


\newcommand{\change@uppercase@math}{%
  \count@=`\A
  \loop
    \mathcode\count@\count@
    \ifnum\count@<`\Z
    \advance\count@\@ne
  \repeat}

\newcommand{\LSTM}[1]{
  \mathrm{LSTM}(
  %(\begingroup\change@uppercase@math#1\endgroup)
}

\newcommand{\UPDATE}[1]{
  \mathrm{UPDATE}(
  %(\begingroup\change@uppercase@math#1\endgroup)
}

\newcommand{\READ}[1]{
  \mathrm{READ}(
 % (\begingroup\change@uppercase@math#1\endgroup)
}

\newcommand{\ADD}[1]{
  \mathrm{ADD}(
 % (\begingroup\change@uppercase@math#1\endgroup)
}
\newcommand{\MUL}[1]{
  \mathrm{MUL}(
 % (\begingroup\change@uppercase@math#1\endgroup)
}

\newcommand{\CIRC}[1]{
  \mathrm{circ}(
 % (\begingroup\change@uppercase@math#1\endgroup)
}

\newcommand{\ReLU}[1]{
  \mathrm{ReLU}(
 % (\begingroup\change@uppercase@math#1\endgroup)
}


\newcommand{\softmax}[1]{
  \mathrm{softmax}(
 % (\begingroup\change@uppercase@math#1\endgroup)
}


\makeatother

\newcommand*\GetListMember[2]{\StrBetween[#2,\number\numexpr#2+1]{,#1,},,\par}%
\newcommand{\tikzmark}[1]{\tikz[overlay,remember picture] \node (#1) {};}

\def\spvec#1{\left(\vcenter{\halign{\hfil$##$\hfil\cr \spvecA#1;;}}\right)}
\def\spvecA#1;{\if;#1;\else #1\cr \expandafter \spvecA \fi}

\newlength{\MidRadius}
\newcommand*{\CircularSequence}[3]{%
    % #1 = outer circle radius
    % #2 = inner circle radius
    % #3 = seqeunce
    \StrCount{#3}{,}[\NumberOfElements]
    \pgfmathsetmacro{\AngleSep}{360/(\NumberOfElements+1)}
    \pgfmathsetlength{\MidRadius}{(#1+#2)/2}
    \draw [red,  ultra thick] circle (#2);
    \draw [blue, ultra thick] circle (#1);
%    \draw [thick,->] (0, 0) -- (1.0, 0);
%    \draw [thick,->] (0, 0) -- (0.0, 1.0);
    \foreach [count = \Count] \Angle in {0,\AngleSep,..., 360} {%
        \draw [gray, ultra thick] (\Angle:#2) -- (\Angle:#1);
        \pgfmathsetmacro{\MidPoint}{\Angle+\AngleSep/2}
        \node at (\MidPoint:\MidRadius) {\GetListMember{#3}{\Count}};
    }%7
}%


\author{Povilas Daniu\v{s}is}
\author[1]{Povilas Daniu\v{s}is \thanks{povilas.daniusis@gmail.com}}


\title{Bilinear recurrent neural networks (draft)}

\begin{document}
\maketitle
\section{Introduction}

Many sequence learning problems are efficiently solved by recurrent neural networks (RNN's) (e.g. speech recognition \cite{Sak}, machine translation \cite{Sutskever}, image caption generation \cite{Vinyals}). RNN's encode a sequence of inputs into state variable of fixed structure, which is further used to infer a sequence of corresponding outputs. 



Although theoretically RNN's are capable of performing arbitrary computations \cite{Siegelmann}, in practice training of RNN's is often non-trivial and time consuming process, and therefore new, more efficient architectures and training methods are needed.



Motivated by successful applications of bilinear projections in
various machine learning methods (e.g. bilinear hashing \cite{Gong}, feed forward neural nets \cite{Daniusis}, SVM's \cite{Cai}), we devote this study to an analysis of their effectiveness in RNN's.


This article contributes to RNN's by applying bilinear products to derive new modification of long short-term memory (LSTM) \cite{Hochreiter} and gated recurrent unit (GRU) \cite{Chung} recurrent neural networks, and conducting an empirical analysis of suggested models. LSTM and GRU were chosen because of their effectiveness.

Main advantages of our approach are that it can be used directly for matrix-valued sequences, is able to capture the structure of such a data more efficiently comparing to conventional analogues, and contains less parameters. Tensorflow \cite{Tensorflow} implementation of suggested bilinear RNN's can be downloaded from \url{https://github.com/povidanius/bilinear_rnn}.




\subsection{Short review of RNN models}

Let $t$ be discrete time variable, and let $x_{t} \in \mathbb{R}^{D_{x}}$ be corresponding input vectors. Simple RNN (SRNN) model is defined by 
recurrence 

\begin{equation}
\label{eq:SRNN}
h_{t} = \phi(Wx_{t} + Uh_{t-1} + b),
\end{equation}

\noindent where $h_{t}$ is state vector, $W$ and $U$ are parameter matrices, $b$ is bias vector, and $\phi$ - activation function \cite{Elman}. 
Receiving the sequence of inputs $x_{t}$, the model maintains  state $h_{t}$, and 
outputs $y_{t} = \psi(Vh_{t} + c)$, where $V$ and $c$ are another parameters, and $\psi$ is output activation function. Although SRNN is able to learn non-trivial sequential regularities, in practice it is rather limited. The limitations of SRNN arise from so called gradien vanishing/exploding effect~\cite{Bengio}.



\subsubsection{LSTM}


LSTM \cite{Hochreiter} ameliorates aforementioned drawbacks of SRNN by refining the state update scheme. It relies on the combination of two innovations: gate mechanism and additive state updates. The state  variable of LSTM is a pair of vectors $(c_{t}, h_{t})$, which may be interpreted as "long" and "short" type memories. 


After receiving new input $x_{t}$ the LSTM combines it with the $h_{t-1}$ into new candidate memory $
\tilde{c}_{t}$, and constructs gates $i_{t}$, $f_{t}$ and $o_{t}$. 

The input gate $i_{t}$ controls the integration of candidate $\tilde{c}_{t}$ into $c_{t}$, allowing to 
inhibit or activate certain components of candidate cell. Similarly, the forget gate $f_{t}$ controls integration or previous memory $c_{t-1}$, 
and output gate $o_{t}$ multiplicatively exposes $c_{t}$ into new hidden state $h_{t}$.

Formally, LSTM model is defined by:

\begin{align}
\begin{split}
\label{LSTM}
i_{t} &=\sigma(W^{i}x_{t} + U^{i}h_{t-1} + b^{i})\\
f_{t} &=\sigma(W^{f}x_{t} + U^{f}h_{t-1} + b^{f})\\
o_{t} &=\sigma(W^{o}x_{t} + U^{o}h_{t-1} + b^{o})\\
\tilde{c}_{t} &= \tanh(W^{c}x_{t} + U^{c}h_{t-1} + b^{c})\\
c_{t} &=f_{t} \bullet c_{t-1} + i_{t} \bullet \tilde{c}_{t}\\
h_{t} &=o_{t} \bullet \tanh(c_{t}), \qedhere
\end{split},
\end{align}

\noindent where $\bullet$ denotes element-wise multiplication,  $\sigma$ and $\tanh$ denotes sigmoid and hyperbolic tangent. All the gates and state variables are $D_{h}$ - dimensional vectors. LSTM is defined by $4 \cdot D_{h} \cdot (D_{x}  + D_{h} + 1)$, and state is defined by $2 \cdot D_{h}$ variables.

%Linearly changing $c_{t}$ may be interpreted as "long" memory, while fast changing $h_{t}$ represents "short" memory.

%$c_{t}$ may be interpreted as internal memory of LSTM, while $h_{t}$ - represent its content, exposed by the output gate.

\subsubsection{GRU}
Similar and simpler recurrent architecture, gated recurrent unit (GRU) \cite{Chung}, is defined by:

\begin{align}
\begin{split}
\label{LSTM}
u_{t} &=\sigma(W^{i}x_{t} + U^{i}h_{t-1} + b^{i})\\
r_{t} &=\sigma(W^{f}x_{t} + U^{f}h_{t-1} + b^{f})\\
\tilde{h}_{t} &= \tanh(W^{h}x_{t} + r_{t} \bullet (U^{h}h_{t-1}) + b^{h})\\
h_{t} &= u_{t} \bullet \tilde{h}_{t} + (1 - u_{t}) \bullet h_{t-1}, \qedhere
\end{split}
\end{align}

\noindent Reset gate $r_{t}$ controls integration of previous state $h_{t-1}$ into candidate state $\tilde{h}_{t}$, and update gate $u_{t}$ controls integration of candidate state into state update. GRU has $ 3 \cdot D_{h} \cdot (D_{x}  + D_{h} + 1) $ parameters and $D_{h}$-dimensional state variable. The effectiveness of GRU and LSTM is often very similar.




\subsection{Bilinear analogues of LSTM and GRU}

Linear projections plays fundamental role in many machine learning algorithms. However, in its conventional form it ignores the internal structure of the data, which may be important.

This section describes bilinear LSTM and GRU RNN's. We assume that the inputs $X_{t}$ are $D_{X}^{1} \times D_{X}^{2}$ matrices, and hidden state variables are $D_{H}^{1} \times D_{H}^{2}$ matrices. Bilinear SRNN can now be reformulated as:
$H_{t} = \phi(W_{1}X_{t}W_{2} + U_{1}H_{t-1}U_{2} + B)$, where $\phi$ is non-linear activation function, and parameter matrices $W_{1}$,$W_{2}$,$U_{1}$, $U_{2}$ and $B$ of appropriate dimensions.

We expect that such an approach allows to capture linear row and column structures of the data, and is more compact in terms of parameters (see Table~\ref{table:parameters_and_states}).


\subsubsection{Bilinear LSTM (BLSTM)} 
\begin{align}
\begin{split}
\label{BLSTM}
I_{t} &=\sigma(W_{1}^{i}X_{t}W_{2}^{i} + U_{1}^{i}H_{t-1}U_{2}^{i}  +  B^{i})\\
F_{t} &=\sigma(W_{1}^{f}X_{t}W_{2}^{f} + U_{1}^{f}H_{t-1}U_{2}^{f}  +  B^{f})\\
O_{t} &=\sigma(W_{1}^{o}X_{t}W_{2}^{o} + U_{o}^{i}H_{t-1}U_{2}^{o}  +  B^{o})\\
\tilde{C}_{t} &=\tanh(W_{1}^{c}X_{t}W_{2}^{c} + U_{1}^{c}H_{t-1}U_{2}^{c}  +  B^{c})\\
C_{t} &= F_{t} \bullet C_{t-1} + I_{t} \bullet \tilde{C}_{t}\\
H_{t} &= O_{t}\bullet \tanh(C_{t}),
\end{split}.
\end{align}



\subsubsection{Bilinear GRU (BGRU)} 
\begin{align}
\begin{split}
\label{BGRU}
R_{t} &=\sigma(W_{1}^{i}X_{t}W_{2}^{i} + U_{1}^{i}H_{t-1}U_{2}^{i}  +  B^{i})\\
U_{t} &=\sigma(W_{1}^{f}X_{t}W_{2}^{f} + U_{1}^{f}H_{t-1}U_{2}^{f}  +  B^{f})\\
\tilde{H}_{t} &= th(W_{1}^{c}X_{t}W_{2}^{c} + R_{T} \bullet (U_{1}^{c}H_{t-1}U_{2}^{c})  +  B^{c})\\
H_{t} &= U_{t}\bullet \tilde{H}_{t} + (1_{D_{H}^{1}}1_{D_{H}^{2}}^{T} - U_{t})\bullet H_{t-1},
\end{split}.
\end{align}

\noindent where $1_{d} \in \mathbb{R}^{d}$ is column vector of ones. All the variables in Eq.~\eqref{BLSTM} and Eq.~\eqref{BGRU} are $D_{H}^{1} \times D_{H}^{2}$ dimensional matrices.

%\subsubsection{Analysis}

\begin{table}
\begin{tabular}{ |l|l|l| }
  \hline
   Method & Parameter dimension & State dimension \\
  \hline   
   LSTM & $ 4 \cdot D_{h} \cdot (D_{x}  + D_{h} + 1)$  & $2D_{h}$ \\
  \hline   
   BLSTM & 	$4 \dot ( D_{H}^{1} (D_{X}^{1} + D_{H}^{1} + D_{H}^{2}) + D_{H}^{2} (D_{X}^{2} + D_{H}^{2}))$ & $2 D_{H}^{1} D_{H}^{2}$\\
  \hline   
   GRU & $ 3 \cdot D_{h} \cdot (D_{x}  + D_{h} + 1) $ & $D_{h}$ \\      
  \hline   
   BGRU & 	$3 \dot ( D_{H}^{1} (D_{X}^{1} + D_{H}^{1} + D_{H}^{2}) + D_{H}^{2} (D_{X}^{2} + D_{H}^{2}))$ & $ D_{H}^{1} D_{H}^{2}$\\
  \hline
\end{tabular}
  \caption{Parameter and state space dimensionalities}
\label{table:parameters_and_states}
\end{table}




\section{Computer experiments}

This section is devoted to computer experiments of suggested bilinear RNN's.


%\subsection{Approximation via LSTM}
%We generate 
%\subsection{Meta-learning of regularizer}
%\subsection{Meta-learning of RNN}
%In this experiment we follow scheme described in \cite{Andrychowicz} and try to learn parameters of conventional LSTM using BLSTM.
% Hyper RNN?







\begin{thebibliography}{1}



\bibitem{Andrychowicz} Andrychowicz, M.,  Denil, M., Gomez, S., Hoffman, M. W., Pfau, D., Schaul, T., and de Freitas N.  Learning to learn by gradient descent by gradient descent. CoRR, abs/1606.04474, 2016.

\bibitem{Bengio} Bengio,  Y.,  Simard,  P.,  and  Frasconi, P. Learning long-term dependencies with gradient descent is difficult. Neural Networks, IEEE Transactions on, 5(2):157-166, 1994.


\bibitem{Cai} Cai, D., He, X., Han, J. Learning with Tensor Representation, preprint, 2006.

\bibitem{Cheng} Cheng, Y., Yu, F. X., Feris, R.,S., Kumar, S., Choudhary, A., and Chang, S.  An exploration of parameter redundancy in deep networks with circulant projections. In
ICCV, 2015.


\bibitem{Chung} Chung, J., Gulcehre, C., Cho, K., and Bengio, Y. Empirical evaluation of gated recurrent neural networks on sequence modeling. NIPS Deep Learning Workshop, 2014.

\bibitem{Daniusis} Daniu\v{s}is, P., and Vaitkus, Pr. Neural network with matrix inputs. Informatica, 2008, vol.19 (4): 477-486.


\bibitem{Elman} Elman, J. L. Finding structure in time.  CRL Technical Report 8801, Center for Research in Language, University
of California, San Diego, 1988.

\bibitem{Graves}  Graves, A., Wayne, G., Reynolds, M.,  Harley, T., Danihelka, I., Grabska-Barwi
 nska, A., Colmenarejo, S.G., Grefenstette, E., Ramalho, T., Agapiou, J., et al. Hybrid computing using a neural network with dynamic external memory. Nature, 2016.
 


\bibitem{Gong}  Gong, Y., Kumar, S., Rowley, H. A., and Lazebnik, S. Learning binary codes for high-dimensional data using bilinear projections. CVPR, pages 484-491, 2013.



\bibitem{Haykin} Haykin, S. Neural Networks: A Comprehensive Foundation. 2nd Edition. Prentice Hall, 1998.


\bibitem{Henriques} Henriques,  J. F.,  Caseiro, R.,  Martins, P.,  and Batista, J.   Exploiting the Circulant Structure of Tracking-by-Detection with Kernels.   In ECCV, 2012.


\bibitem{Hochreiter} Hochreiter, S.  and  Schmidhuber J. Long  Short-Term  Memory. Neural  Computation, 9(8): pp. 1735-1780, 1997.



%\bibitem{Kingma} Kingma,  D.,  and  Ba,  J.
%Adam:   A  Method  for Stochastic Optimization. arXiv:1412.6980 [cs.LG], December 2014.


\bibitem{Oppenheim} Oppenheim,  A.  V.,  Schafer, R. W.,  Buck,  J. R. Discrete-time signal processing, volume 5.  Prentice Hall Upper Saddle River, 1999.

\bibitem{Pedregosa} Pedregosa, F., Varoquaux, G.,  Gramfort, A.,  Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,  Weiss, R., Dubourg, V.,  Vander-plas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. Scikit-learn: Machine Learning in Python, Journal of Machine Learning Research, vol. 12, pp. 2825-2830, 2011.


\bibitem{Sak} Sak, H., Senior, A., Rao, K. and Beaufays, F. Fast and Accurate Recurrent Neural Network Acoustic Models for Speech Recognition. In INTERSPEECH, 2015.

\bibitem{Schmidhuber}  Schmidhuber, J. Deep  learning  in  neural  networks:  An  overview. Neural  Networks,  61: 85-117, 2015.

%\bibitem{Srivastava} Srivastava, N., Hinton, G.E., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R.R. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929-1958, 2014.

\bibitem{Siegelmann} Siegelmann, H. T., and Sontag, E. D.  On
the  computational  power  of  neural  nets.
Journal of Computer and System Sciences, 50, 1995.




\bibitem{Sutskever} Sutskever, I., Vinyals, O., and Le, Q. V.. Sequence to sequence learning with neural networks, pp. NIPS 2014.


\bibitem{Tensorflow}  Abadi M., Agarwal A., Barham P., Brevdo E., Chen Z., Citro C., Corrado G., Davis A., Dean J., Devin M., et al. TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems. 2016. arXiv:1603.04467



\bibitem{Tieleman} Tieleman, T., and Hinton, G.   Lecture 6.5-rmsprop:  Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4, 2012.


\bibitem{Vinyals} Vinyals, O., Bengio, S., Erhan, D. Show and tell: A neural image caption generator. In CVPR, 2015.




\bibitem{Yu} Yu, F.,  Kumar, S., Gong, Y, and Chang, S.,-F.  Circulant binary embedding. In ICML, Beijing, China, 2014, pp. 946-954.

\bibitem{Yu0}  Yu, F.  X.,  Kumar, S.,   Rowley, H.,  and  Chang, S.-F. Compact nonlinear maps and circulant extensions. ArXiv preprint arXiv:1503.03893, 2015.



\bibitem{Zhang} Zhang, M., McCarthy, Z., Finn, C., Levine, S. and Abbeel, P. Learning deep neural network policies with continuous memory states, in IEEE International Conference on Robotics and Automation (ICRA), May 2016, pp. 520-527.




\end{thebibliography}



\end{document}